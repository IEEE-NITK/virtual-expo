<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
    <title>Adversarial Reinforcement Learning | Virtual Expo</title>
    <meta name="generator" content="Jekyll v3.8.7" />
    <meta property="og:title" content="Adversarial Reinforcement Learning" />
    <meta name="author" content="IEEE NITK" />
    <meta property="og:locale" content="en_US" />
    <meta name="description" content="Introduction and Background" />
    <meta property="og:description" content="Introduction and Background" />
    <link rel="canonical" href="https://ieee.nitk.ac.in//virtual-expo/adversarial-reinforcement-learning/" />
    <meta property="og:url" content="https://ieee.nitk.ac.in//virtual-expo/adversarial-reinforcement-learning/" />
    <meta property="og:site_name" content="Virtual Expo" />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2020-07-01T00:00:00+05:30" />
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167549289-3"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-167549289-3');
    </script>
    <!-- gtag end -->
    <script type="application/ld+json">
            {"url":"https://ieee.nitk.ac.in//virtual-expo/adversarial-reinforcement-learning/","headline":"Adversarial Reinforcement Learning","dateModified":"2020-07-01T00:00:00+05:30","datePublished":"2020-07-01T00:00:00+05:30","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://ieee.nitk.ac.in//virtual-expo/adversarial-reinforcement-learning/"},"author":{"@type":"Person","name":"IEEE NITK"},"description":"Introduction and Background","@context":"https://schema.org"}</script>
    <!-- End Jekyll SEO tag -->
    <link rel="apple-touch-icon" sizes="180x180" href="/virtual-expo/assets/img/icons/apple-touch-icon.webp">
    <link rel="icon" type="image/png" href="/virtual-expo/favicon.ico">
    <link rel="icon" type="image/webp" sizes="16x16" href="/virtual-expo/favicon.ico">
    <link rel="mask-icon" href="/virtual-expo/assets/img/icons/safari-pinned-tab.svg" color="#5bbad5">
    <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico"><![endif]-->
    <link rel="shortcut icon" href="/virtual-expo/favicon.ico">
    <meta name="apple-mobile-web-app-title" content="IEEE NITK Virtual Expo">
    <meta name="application-name" content="IEEE NITK Virtual Expo">
    <meta name="msapplication-config" content="/virtual-expo/assets/img/icons/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <style class="inlineCSS">
        h1 {
            color: #313237;
            margin-top: 0;
            margin-bottom: 0.5rem
        }

        .dark-bg {
            background-color: #313237
        }

        @media only screen and(min-width:48em) {
            .post-card {
                width: 48.4375%;
                margin-right: 3.125%
            }

            .post-card:last-of-type,
            .post-card:nth-child(2n + 2) {
                margin-right: 0
            }
        }

        html {
            line-height: 1.15;
            -ms-text-size-adjust: 100%;
            -webkit-text-size-adjust: 100%
        }

        body {
            margin: 0
        }

        header,
        nav,
        section {
            display: block
        }

        h1 {
            font-size: 2em;
            margin: 0.67em 0
        }

        figure,
        main {
            display: block
        }

        figure {
            margin: 1em 40px
        }

        a {
            background-color: transparent;
            -webkit-text-decoration-skip: objects
        }

        img {
            border-style: none
        }

        svg:not(:root) {
            overflow: hidden
        }

        ::-webkit-file-upload-button {
            -webkit-appearance: button;
            font: inherit
        }

        html {
            -webkit-box-sizing: border-box;
            box-sizing: border-box
        }

        body {
            -webkit-overflow-scrolling: touch
        }

        *,
        ::after,
        ::before {
            -webkit-box-sizing: inherit;
            box-sizing: inherit
        }

        .site {
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            min-height: 100vh;
            -webkit-box-orient: vertical;
            -webkit-box-direction: normal;
            -ms-flex-direction: column;
            flex-direction: column
        }

        .site__content {
            -webkit-box-flex: 1;
            -ms-flex: 1;
            flex: 1
        }

        img {
            max-width: 100%;
            height: auto;
            width: auto;
            vertical-align: middle
        }

        figure {
            margin: 0
        }

        body {
            background-color: #fff;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Hiragino Sans GB", "Microsoft YaHei", "WenQuanYi Micro Hei", sans-serif;
            font-size: 1rem;
            line-height: 1.5;
            color: #343851;
            -webkit-font-smoothing: antialiased;
            -webkit-text-size-adjust: 100%
        }

        p {
            margin-top: 0;
            margin-bottom: 1.25rem
        }

        h1,
        h2 {
            color: #313237;
            margin-top: 0;
            margin-bottom: 0.5rem
        }

        a {
            color: #277cea;
            text-decoration: none;
            border-bottom: 1px dashed #277cea
        }

        .blur {
            background: #fff;
            filter: url('data:image/svg+xml;charset=utf-8, <svg xmlns="http://www.w3.org / 2000/svg"><filter id="filter"><feGaussianBlur stdDeviation="16" /></filter></svg>#filter');
            -webkit-filter: blur(1rem);
            filter: blur(1rem)
        }

        .container {
            padding: 0 20px;
            max-width: 100%;
            margin: 0 auto
        }

        @media only screen and(min-width:36em) {
            .container {
                max-width: 540px;
                margin: 0 auto
            }
        }

        @media only screen and(min-width:48em) {
            .container {
                max-width: 720px;
                margin: 0 auto
            }
        }

        @media only screen and(min-width:62em) {
            .container {
                max-width: 960px;
                margin: 0 auto
            }
        }

        @media only screen and(min-width:75em) {
            .container {
                max-width: 1170px;
                margin: 0 auto
            }
        }

        .header {
            background-color: #fff;
            color: #343851;
            position: absolute;
            z-index: 4;
            width: 100%;
            top: 0;
            left: 0;
            will-change: transform;
            -webkit-transform: translateY(0);
            transform: translateY(0)
        }

        .header a {
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            -webkit-box-align: center;
            -ms-flex-align: center;
            align-items: center;
            border-bottom: 0
        }

        .header__logo {
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            height: 100%;
            overflow: hidden;
            padding: 19px 0;
            margin-right: 1.25rem;
            outline: 0;
            border-bottom: 0;
            color: #313237
        }

        .header__logo .header__logo--container {
            width: 58px
        }

        .header__logo .header__logo--container .logo {
            fill: currentColor
        }

        .header__inner {
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            -webkit-box-align: center;
            -ms-flex-align: center;
            align-items: center;
            height: 3.75em;
            -webkit-box-pack: justify;
            -ms-flex-pack: justify;
            justify-content: space-between
        }

        .header__links {
            padding-bottom: 0.5rem;
            display: none;
            position: absolute;
            top: 3.75em;
            left: 0;
            width: 100%;
            height: auto;
            background: #fff
        }

        .header__link {
            color: #343851;
            padding: 0.938rem 0;
            border-top: 1px solid #ededed
        }

        .header__toggle {
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            -webkit-box-orient: vertical;
            -webkit-box-direction: normal;
            -ms-flex-direction: column;
            flex-direction: column;
            -webkit-box-pack: center;
            -ms-flex-pack: center;
            justify-content: center;
            width: 44px;
            height: 100%;
            background-color: transparent;
            padding-left: 1.25rem
        }

        .header__toggle span {
            display: block;
            position: relative;
            margin-top: 4px;
            background-color: #343851;
            width: 100%;
            height: 2px;
            border-radius: 1px
        }

        .header__toggle span:first-child {
            margin-top: 0
        }

        @media(min-width:62em) {
            .header__toggle {
                display: none;
                visibility: hidden
            }

            .header__links {
                position: static;
                padding: 0;
                display: -webkit-box;
                display: -ms-flexbox;
                display: flex;
                -webkit-box-orient: vertical;
                -webkit-box-direction: normal;
                -ms-flex-direction: column;
                flex-direction: column;
                visibility: visible;
                width: auto;
                height: 100%
            }

            .header__links-wrapper {
                display: -webkit-box;
                display: -ms-flexbox;
                display: flex;
                height: 100%;
                padding: 0
            }

            .header__link {
                position: relative;
                padding: 0.938rem 1rem;
                border: 0;
                height: 100%
            }

            .header__link::after {
                content: "";
                display: block;
                position: absolute;
                left: 0;
                bottom: 0;
                height: 3px;
                width: 100%;
                -webkit-transform: scaleX(0);
                transform: scaleX(0);
                background: #277cea
            }
        }

        .post-card {
            display: block;
            position: relative;
            width: 100%;
            min-height: 250px;
            border-radius: 4px;
            overflow: hidden;
            background-color: #fff;
            -webkit-box-shadow: 0 1px 3px rgba(0, 0, 0, .08);
            box-shadow: 0 1px 3px rgba(0, 0, 0, .08);
            margin-bottom: 2.25rem;
            border-bottom: 0
        }

        @media only screen and(min-width:48em) {
            .post-card {
                width: 48.4375%;
                margin-right: 3.125%
            }

            .post-card:nth-child(2n + 2) {
                margin-right: 0
            }
        }

        @media only screen and(min-width:75em) {
            .post-card {
                width: 31.25%;
                margin-right: 3.125%
            }

            .post-card:nth-child(2n + 2) {
                margin-right: 3.125%
            }
        }

        .post-card__label {
            position: absolute;
            top: 1.5rem;
            left: 1.5rem;
            z-index: 2
        }

        .post-card__inner {
            display: block;
            position: relative;
            padding: 1.875rem 1.25rem 0.625rem;
            width: 100%;
            color: #838c8d;
            border-bottom: 0
        }

        .post-card__header {
            margin-bottom: 0.75rem
        }

        .post-card__meta {
            font-size: 0.875rem
        }

        .post-card__thumb {
            margin: 0;
            background: #fff;
            position: relative;
            overflow: hidden
        }

        .post-card__thumb::after {
            content: "";
            display: block;
            height: 0;
            width: 100%;
            padding-bottom: 56.25%
        }

        .post-card__thumb>* {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: block
        }

        .label {
            padding: 0 10px;
            margin-bottom: 1rem;
            display: inline-block;
            line-height: 20px;
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: rgba(255, 255, 255, .8);
            border: 2px solid rgba(255, 255, 255, .5);
            border-radius: 100px
        }

        .hero {
            margin: 3.75rem auto 0;
            min-height: 16.25rem;
            width: 100%;
            position: relative;
            background-color: #dde5ea;
            background-repeat: no-repeat;
            background-position: 50%;
            background-size: cover
        }

        @media only screen and(min-width:62em) {
            .hero {
                margin: 0 auto;
                height: 36em
            }
        }

        .hero::before {
            position: absolute;
            display: block;
            content: "";
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(52, 56, 81, .8)
        }

        .hero__wrap {
            position: absolute;
            margin: auto;
            top: 50%;
            left: 50%;
            -webkit-transform: translate(-50%, -50%);
            transform: translate(-50%, -50%);
            text-align: center;
            color: rgba(255, 255, 255, .8);
            width: 100%;
            max-width: 90%;
            z-index: 1
        }

        .hero__wrap .hero__title {
            font-size: 1.8em;
            color: #fff
        }

        .blog {
            background-color: #f9f9f9
        }

        .post-list {
            padding-top: 2.5em;
            display: -webkit-box;
            display: -ms-flexbox;
            display: flex;
            -ms-flex-wrap: wrap;
            flex-wrap: wrap;
            -webkit-box-flex: 1;
            -ms-flex: 1 0 auto;
            flex: 1 0 auto
        }

        @media only screen and(min-width:48em) {
            .hero__wrap {
                max-width: 40em
            }

            .hero__wrap .hero__title {
                padding: 1rem 0;
                font-size: 2.625em;
                line-height: 3.125rem
            }

            .post-list {
                padding-top: 5em
            }
        }
    </style>
    <link rel="preload" href="/virtual-expo/assets/css/main.css" as="style" onload="this.rel='stylesheet'">
    <noscript>
        <link rel="stylesheet" href="/virtual-expo/assets/css/main.css"></noscript>
    <script type="text/javascript">
        /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
        (function (w) {
            "use strict";
            if (!w.loadCSS) {
                w.loadCSS = function () { }
            }
            var rp = loadCSS.relpreload = {};
            rp.support = (function () {
                var ret;
                try {
                    ret = w.document.createElement("link").relList.supports("preload")
                } catch (e) {
                    ret = !1
                }
                return function () {
                    return ret
                }
            })();
            rp.bindMediaToggle = function (link) {
                var finalMedia = link.media || "all";
                function enableStylesheet() {
                    link.media = finalMedia
                }
                if (link.addEventListener) {
                    link.addEventListener("load", enableStylesheet)
                } else if (link.attachEvent) {
                    link.attachEvent("onload", enableStylesheet)
                }
                setTimeout(function () {
                    link.rel = "stylesheet";
                    link.media = "only x"
                });
                setTimeout(enableStylesheet, 3000)
            };
            rp.poly = function () {
                if (rp.support()) {
                    return
                }
                var links = w.document.getElementsByTagName("link");
                for (var i = 0; i < links.length; i++) {
                    var link = links[i];
                    if (link.rel === "preload" && link.getAttribute("as") === "style" && !link.getAttribute("data-loadcss")) {
                        link.setAttribute("data-loadcss", !0);
                        rp.bindMediaToggle(link)
                    }
                }
            };
            if (!rp.support()) {
                rp.poly();
                var run = w.setInterval(rp.poly, 500);
                if (w.addEventListener) {
                    w.addEventListener("load", function () {
                        rp.poly();
                        w.clearInterval(run)
                    })
                } else if (w.attachEvent) {
                    w.attachEvent("onload", function () {
                        rp.poly();
                        w.clearInterval(run)
                    })
                }
            }
            if (typeof exports !== "undefined") {
                exports.loadCSS = loadCSS
            } else {
                w.loadCSS = loadCSS
            }
        }(typeof global !== "undefined" ? global : this))
    </script>
</head>

<body class="site">
    <header class="header" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
        <div class="container">
            <div class="header__inner">
                <a class="header__logo" href="/virtual-expo/">
                    <div class="header__logo--container">
                        <img src="../logo.png" alt="IEEE NITK Logo"></div>
                </a>
                <nav class="header__links">
                    <div class="container header__links-wrapper">
                        <a class="header__link" href="/virtual-expo/" itemprop="url">
                            <span itemprop="name">All Projects</span>
                        </a>
                        <a class="header__link" href="https://ieee.nitk.ac.in/" itemprop="url">
                            <span itemprop="name">Main Website</span>
                        </a>
                    </div>
                </nav>
                <div class="header__toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
    </header>
    <div class="hero lazyload"
        data-bg="https://ieee.nitk.ac.in//virtual-expo/assets/img/adversarial-reinforcement-learning/embed.webp">
        <div class="hero__wrap">
            <div class="hero__categories">
                <a class="label" href="/virtual-expo/compsoc">CompSoc</a>
            </div>
            <h1 class="hero__title">Adversarial Reinforcement Learning</h1>
            <p class="hero__meta">
                <span>
                    <time>01 Jul 2020</time>&nbsp;&middot;
                </span>
                <span>
                    10 mins read
                </span>
            </p>
        </div>
    </div>
    <main class="site__content">
        <div class="container">
            <article class="post-content" itemprop="articleBody">
                <h2 id="team">Team</h2>
                <p>$3^{rd}$ years -</p>
                <ul>
                    <li>Madhuparna Bhowmik</li>
                    <li>Akash Nair</li>
                    <li>Saurabh Agarwala</li>
                </ul>
                <p>$2^{nd}$ years -</p>
                <ul>
                    <li>Videh Raj Nema</li>
                    <li>Kinshuk Kashyap</li>
                    <li>Manav Singhal</li>
                </ul>
                <p>Mentored by -</p>
                <ul>
                    <li>Moksh Jain</li>
                </ul>
                <h2 id="introduction-and-background">Introduction and Background</h2>
                <h3 id="reinforcement-learning">Reinforcement Learning</h3>
                <p>Reinforcement Learning (RL) is a branch of Machine Learning based on learning from rewards and
                    punishments. It involves a feedback loop between the agent(s) and the environment, bounded by a
                    mathematical framework. The objective of the agent(s) is to
                    <strong>learn</strong>
                    a function(s) that exhibits desired behavior in an environment.</p>
                <p><img src="../assets/img/adversarial-reinforcement-learning/rl_loop.webp" alt="RL loop" /></p>
                <p>This is a very simple organization of the RL framework and can become very complicated when it comes
                    to applications in real-world settings. There can be multiple issues like using function
                    approximators, multiple agents, adversaries, a poor model of the world, stability and scalability
                    issues, etc.</p>
                <p>We aim to explore one such direction in this project.</p>
                <h3 id="deep-reinforcement-learning">Deep Reinforcement Learning</h3>
                <p>The scalable applications of Reinforcement Learning are accomplished by blending RL methods with the
                    representational capacity of Deep Learning models like
                    <strong>Neural Nets</strong>. The state and action space of the environment can be exponentially
                    large in the real-world settings, and hence the traditional Tabular RL framework cannot be used,
                    even with an asymptotically infinite amount of computation power available.</p>
                <p>Hence we make the use of function approximators (specifically Neural Nets) to learn the functions
                    like the
                    <em>policies</em>
                    and the
                    <em>value-functions</em>. These functions essentially correspond to the “behavior” of the agent(s)
                    in the environment.</p>
                <p>As we all know, Neural Nets are extremely good at approximating functions and learning complex
                    behaviors from its applications in other Deep Learning domains like Computer Vision and NLP. They
                    prove to be effective even in RL settings, though the learning process needs certain
                    <em>bells &amp; whistles</em>
                    to work efficiently and effectively.</p>
                <p>There have been a large number of wide and successful applications of Deep RL in the recent years
                    ranging from reaching super human-level performance in playing games like
                    <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">Go</a>,
                    <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">Chess</a>,
                    <a
                        href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">Atari</a>,
                    <a href="https://openai.com/projects/five/">Dota2</a>, etc.,
                    <a href="https://arxiv.org/abs/1909.03602">Online advertising</a>, applications in Medicine for
                    <a href="https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery">Protein
                        Folding</a>,
                    <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">robotics</a>,
                    and many more. The list goes on!</p>
                <p>
                    <strong>But are the systems robust enough? Can they be failed? What vulnerabilities do they have? Do
                        they really achieve super human-level performance?</strong>
                </p>
                <p>Let us elucidate upon this question with an example from Supervised Learning -</p>
                <p><img src="../assets/img/adversarial-reinforcement-learning/fgsm.webp" alt="FGSM on images" /></p>
                <p>This is a work from
                    <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al., 2015</a>
                    and shows the existence of
                    <strong>Adversarial examples</strong>
                    in Computer Vision. Given is an image of
                    <em>Panda</em>, which is classified correctly with a probability of around $57.7\%$. The middle of
                    the figure seems the addition of seemingly random
                    <em>noise</em>
                    times a small constant factor to the original image, and the resulting image seems to be almost the
                    same as the original. Still, it is classified as a
                    <em>Gibbon</em>
                    with $99.3\%$ confidence by the same Neural Network!!</p>
                <p>This result looks bizarre, but actually is nothing but a construction of adversarial examples for the
                    corresponding clean example. The image in the middle that looks like
                    <em>noise</em>
                    is not actually some random permutation of the pixels; instead
                    <strong>it is a carefully computed perturbation</strong>
                    that is specifically designed for the model to make a mistake.</p>
                <h2 id="objectives">Objectives</h2>
                <p>You might be wondering if such adversarial attacks exist in Reinforcement Learning too, and the
                    answer is
                    <strong>Yes</strong>
                    they do! The objective of this project was to explore Adversarial attacks in Single as well as
                    Multi-Agent domains, and study their effectiveness against existing state-of-the-art (SOTA) works.
                    We also studied Defense mechanisms to resist such attacks.</p>
                <p>We focussed on the following two domains:</p>
                <p>
                    <strong>Pixel-Based attacks</strong>
                    Attacking Deep RL systems that take image pixels as the state (or observation) input. We use Atari
                    2600 games (in particular Pong) as a benchmark to test the attack in White-box as well as Black-box
                    settings. This is inspired directly from the way adversarial examples are generated in Supervised
                    settings.</p>
                <p>
                    <strong>Adversarial Policies</strong>
                    Attacking agents trained with Deep RL methods in Competitive zero-sum continuous control robotic
                    environments from the
                    <a href="http://www.mujoco.org/">MuJoCo</a>
                    simulator. The attack is accomplished via training an Adversarial policy with Black-box access to
                    the environment. This Multi-Agent Thread model closely resembles the real-world attacks.</p>
                <h2 id="pixel-based-attacks">Pixel-Based attacks</h2>
                <p><img src="../assets/img/adversarial-reinforcement-learning/adv_in_loop.webp"
                        alt="Adversary in the loop" /></p>
                <p>This class of adversarial attacks involves the systems where the adversary can be a part of the
                    agent-environment loop. It need not have complete information about the agent/victim. The attacks
                    also exist in Black-box conditions, due to the
                    <a href="https://arxiv.org/abs/1312.6199">Transferability</a>
                    of adversarial examples. We deal with both settings.</p>
                <p>Most of these attacks are studied in Computer Vision but can be applied in a similar fashion in RL,
                    where the input state-space to the Neural Net is the pixels. There are multiple approaches to
                    achieve this with a common goal for all. The goal is to -</p>
                <ul>
                    <li>
                        <p>Find the input features that are most sensitive to class change (change in the action
                            executed by the agent in RL).</p>
                    </li>
                    <li>
                        <p>Make a perturbation, i.e., change the pixels slightly such that the network misclassifies the
                            input, but the change is visually imperceptible to humans. For RL, it would be equivalent to
                            forcing the agent to take the wrong action.</p>
                    </li>
                </ul>
                <p><img src="../assets/img/adversarial-reinforcement-learning/pixel_attacks.webp"
                        alt="Pixel attack on Pong" /></p>
                <p>This an illustration of the
                    <a href="https://arxiv.org/abs/1412.6572">FGSM attack</a>
                    method on the Atari game Pong. First, we train a policy to play Pong with the
                    <a
                        href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/policy-gradient-methods,/sampled-learning,/optimization/theory/2020/03/25/Proximal-Policy-Optimization.html">Proximal
                        Policy Optimization</a>
                    (PPO) algorithm for about 20 Million time steps. The Pong agent becomes extremely good at playing
                    the game and is able to beat the opponent most of the time. Then we apply the attack method (refer
                    to this
                    <a
                        href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/adversarial/attacks,/defense/mechanisms/2020/04/09/Survey-on-Adversarial-attacks-and-defenses.html">post</a>
                    for more details). The results we obtained clearly show the effectiveness of the adversarial attack
                    carried out.</p>
                <p>For further illustration, please see the supplementary
                    <a href="https://www.youtube.com/watch?v=j1Img72wp00">video</a>.</p>
                <h2 id="adversarial-policies">Adversarial Policies</h2>
                <p>The constraint that the adversary can be a part of the agent-environment loop can be unrealistic in
                    certain cases, especially in real-world applications.</p>
                <p><img src="../assets/img/adversarial-reinforcement-learning/threat_model.webp"
                        alt="Multi-agent threat model" /></p>
                <p>Hence we have a more realistic multi-agent threat model, where the adversary influences the other
                    agent or the victim by taking certain adversarial actions, which in turn affect the victim’s
                    observations. So this is different from the pixel attacks where the adversary could perturb the
                    input directly. This is commonly seen in various applications like self-driving cars. In such cases,
                    the adversary cannot directly manipulate the pixels from the camera on the car, or it cannot do
                    things like make a building disappear. Hence this form of attack is applicable in real-world
                    settings.</p>
                <p><img src="../assets/img/adversarial-reinforcement-learning/embed.webp"
                        alt="Simplified environment" /></p>
                <p>We make a simplification by making the victim agent a part of the environment itself. What this means
                    is that we fix the victim’s parameters, and by doing so, we essentially make it a part of the
                    environment. Doing this, now we have a single-agent RL problem, where the agent is the adversary.
                </p>
                <p>And this is not an unrealistic assumption. In common applications of reinforcement learning or
                    machine learning in general, we train a system and then freeze or seal it for deployment. Hence the
                    attack can happen in such a situation.</p>
                <p><img src="../assets/img/adversarial-reinforcement-learning/envs.webp" alt="Environements" /></p>
                <p>These are the environments we consider. Such environments closely resemble real-world settings. All
                    three are created using MuJoCo.
                    <em>Kick-and-Defend</em>
                    is a football type environment called, where the objective of the kicker is to kick the ball into
                    the goal post, and the goalie defends it. Next is an environment called
                    <em>You Shall Not Pass</em>, where the aim of this runner is to cross the finish line with a stable
                    gait, and the blocker aims to stop it. And lastly, a Sumo environment, where the objective of both
                    the agents is to knock the other one out.</p>
                <h4 id="attack-procedure">Attack Procedure</h4>
                <ul>
                    <li>
                        <p>The attack is implemented in a different and more realistic manner here. Unlike the Pixel
                            attacks, the adversary is no longer a part of the agent-environment loop. It can affect the
                            victim only through its actions. We show the results in the Kick-and-Defend environment with
                            the Kicker as a victim and the Goalie as an adversary.</p>
                    </li>
                    <li>
                        <p>We embed the victim in the environment (fix its policy parameters), treating the adversary
                            and the environment as a Single-agent system. Then we train an adversarial policy (MLP and
                            LSTM) with Proximal Policy Optimization (PPO) and Black-box access to the victim policy’s
                            actions, for about 20 Million time steps, which is less than 3% of the time steps used to
                            train the victim agents (via self-play).</p>
                    </li>
                    <li>
                        <p>The adversarial agent is trained with a sparse reward at the end of the episode, positive
                            when it wins the game and negative when it loses or ties.</p>
                    </li>
                </ul>
                <h4 id="observations">Observations</h4>
                <p>Following are some important observations from the attack procedure:</p>
                <ul>
                    <li>
                        <p>Despite being trained for less than 3% of the time steps, the adversary is successfully able
                            to beat the victim with high win-ratio. This is an important point as the policies that were
                            attacked were the SOTA self-play methods in these environments, which are trained explicitly
                            to be robust to all opponents.</p>
                    </li>
                    <li>
                        <p>The adversary does not beat the victim by becoming strong opponents; instead by seemingly
                            random behavior. Also, the same adversarial policy is able to “fool” different victim
                            policies trained with different random seeds.</p>
                    </li>
                    <li>
                        <p>The adversary does not win just because it is an out-of-distribution policy. The adversary
                            wins by taking actions that are naturally adversarial to the victim policy network. The
                            activations recorded in the victim policy network are very different from the usual (<a
                                href="https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95">GMM</a>
                            &amp;
                            <a
                                href="https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1">t-SNE</a>
                            visualization). Clearly, the victim and the opponent are not in
                            <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a>.</p>
                    </li>
                </ul>
                <h2 id="defense-mechanisms">Defense Mechanisms</h2>
                <p>We also studied various Defense methods that have been proposed to resist such attacks. Sadly, there
                    is no single defense system developed yet that can be used
                    <em>effectively</em>
                    against the respective attack methods. There have been a few methods introduced, but either they
                    work only for certain adversarial policies and not against the others, or they do not work against
                    <a
                        href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/adversarial/attacks,/defense/mechanisms/2020/04/09/Survey-on-Adversarial-attacks-and-defenses.html#cw">stronger
                        attacks</a>.</p>
                <p>To know more about the defensive systems, refer
                    <a
                        href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/adversarial/attacks,/defense/mechanisms/2020/04/09/Survey-on-Adversarial-attacks-and-defenses.html#defenses-against-adv">this</a>
                    and
                    <a
                        href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/adversarial/attacks,/defense/mechanisms/2020/04/09/Survey-on-Adversarial-attacks-and-defenses.html#defense-against-adv-policies">this</a>.
                    These were studied as a part of this project.</p>
                <h2 id="results-and-conclusion">Results and Conclusion</h2>
                <p>The existence of such attacks shows the vulnerabilities of function approximators like Neural Nets
                    and forces us to rethink about Generalization in Machine Learning. It is not that these attacks are
                    a characteristic property of Deep Learning models like Neural Nets. They exist even in the most
                    linear Machine Learning models that we see. Since Neural Nets are built on these architectures, they
                    inherit this flaw.</p>
                <p>Also, sometimes it is not the algorithm or the function approximator architecture that might be
                    responsible for adversarial attacks. As we saw in the Adversarial Policies section, the victim and
                    the opponent where clearly not in Nash Equilibrium, because if they would have been, then no
                    adversary would be able to exploit the victim. This suggests that there might be some inherent flaw
                    in the Optimization procedure or the training procedure that causes the occurrence of such attacks.
                </p>
                <p>Nevertheless, analyzing adversarial attacks encourages us to build more robust systems and be
                    prepared not only for a good average-case but also for a good worst-case performance.</p>
                <h2 id="resources">Resources</h2>
                <ul>
                    <li>
                        <p>
                            <a
                                href="https://aarl-ieee-nitk.github.io/reinforcement-learning,/adversarial/attacks,/defense/mechanisms/2020/04/09/Survey-on-Adversarial-attacks-and-defenses.html">Detailed
                                article on Adversarial Attacks and Defenses in Reinforcement Learning</a>
                        </p>
                    </li>
                    <li>
                        <p>
                            <a href="https://github.com/IEEE-NITK/Adversarial-Reinforcement-Learning">Code for the
                                Project</a>
                        </p>
                    </li>
                    <li>
                        <p>
                            <a href="https://aarl-ieee-nitk.github.io/">Blog-site for the Project</a>
                        </p>
                    </li>
                    <li>
                        <p>
                            <a href="https://github.com/aarl-ieee-nitk/aarl-ieee-nitk.github.io">GitHub Organization</a>
                        </p>
                    </li>
                </ul>
                <h2 id="video">Video</h2>
                <iframe width="100%" height="480" src="https://www.youtube.com/embed/j1Img72wp00"
                class="center-video" allowfullscreen></iframe>
            </article>
            <div class="post-content controls__inner">
                <div class="controls__item next">
                    <span>Previous</span>
                    <a href="/virtual-expo/codeshare/">
                        <span>
                            <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                                <path fill="fillColor"
                                    d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z" />
                            </svg>
                        </span>
                        CodeShare
                    </a>
                </div>
                <div class="controls__item prev"> <span>Next</span> <a
                        href="/virtual-expo/contextual-sarcasm-detection/"> Contextual Sarcasm Det... <span>
                            <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                                <path fill="#fillColor"
                                    d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z" />
                            </svg> </span> </a></div>
            </div>
        </div>
    </main>
    <footer class="footer">
        <div class="container">
            <nav class="social">
                <a class="social__link" target="_blank" rel="noopener noreferrer" href="https://github.com/IEEE-NITK">
                    <svg class="social__icon" viewbox="0 0 20 20" width="20px" height="20px">
                        <path
                            d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z" />
                        </svg>
                </a>
                <a class="social__link" target="_blank" rel="noopener noreferrer"
                    href="https://www.instagram.com/ieee_nitk">
                    <svg class="social__icon" viewbox="0 0 20 20" width="20px" height="20px">
                        <path
                            d="M8 0C5.827 0 5.555.01 4.702.048 3.85.088 3.27.222 2.76.42a3.908 3.908 0 0 0-1.417.923c-.445.444-.72.89-.923 1.417-.198.51-.333 1.09-.372 1.942C.008 5.555 0 5.827 0 8s.01 2.445.048 3.298c.04.852.174 1.433.372 1.942.204.526.478.973.923 1.417.444.445.89.72 1.417.923.51.198 1.09.333 1.942.372.853.04 1.125.048 3.298.048s2.445-.01 3.298-.048c.852-.04 1.433-.174 1.942-.372a3.908 3.908 0 0 0 1.417-.923c.445-.444.72-.89.923-1.417.198-.51.333-1.09.372-1.942.04-.853.048-1.125.048-3.298s-.01-2.445-.048-3.298c-.04-.852-.174-1.433-.372-1.942a3.908 3.908 0 0 0-.923-1.417A3.886 3.886 0 0 0 13.24.42c-.51-.198-1.09-.333-1.942-.372C10.445.008 10.173 0 8 0zm0 1.44c2.136 0 2.39.01 3.233.048.78.036 1.203.166 1.485.276.374.145.64.318.92.598.28.28.453.546.598.92.11.282.24.705.276 1.485.038.844.047 1.097.047 3.233s-.01 2.39-.05 3.233c-.04.78-.17 1.203-.28 1.485-.15.374-.32.64-.6.92-.28.28-.55.453-.92.598-.28.11-.71.24-1.49.276-.85.038-1.1.047-3.24.047s-2.39-.01-3.24-.05c-.78-.04-1.21-.17-1.49-.28a2.49 2.49 0 0 1-.92-.6c-.28-.28-.46-.55-.6-.92-.11-.28-.24-.71-.28-1.49-.03-.84-.04-1.1-.04-3.23s.01-2.39.04-3.24c.04-.78.17-1.21.28-1.49.14-.38.32-.64.6-.92.28-.28.54-.46.92-.6.28-.11.7-.24 1.48-.28.85-.03 1.1-.04 3.24-.04zm0 2.452a4.108 4.108 0 1 0 0 8.215 4.108 4.108 0 0 0 0-8.215zm0 6.775a2.667 2.667 0 1 1 0-5.334 2.667 2.667 0 0 1 0 5.334zm5.23-6.937a.96.96 0 1 1-1.92 0 .96.96 0 0 1 1.92 0z">
                        </path>
                    </svg>
                </a>
            </nav>

        </div>
    </footer>
    <script async src="/virtual-expo/assets/js/bundle.js"></script>
    <script async>
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.register('/virtual-expo/sw.js').then(function (registration) {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
            }).catch(function (error) {
                console.log('ServiceWorker registration failed: ', error);
            });
        }
    </script>
    <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
        </script>
    <script type="text/javascript" charset="utf-8"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" charset="utf-8"
        src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>
</body>

</html>