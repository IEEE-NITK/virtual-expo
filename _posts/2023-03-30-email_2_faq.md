---
layout: post
title: "Email-2-FAQ"
description: "Deploying an F-Gen framework model to generate an FAQ from multiple emails"
categories: compsoc
thumbnail: "2023_Email-2-FAQ.gif"
year: 2023
gmeet: "https://meet.google.com/wjc-tqkh-qva"
---

### Mentors:
- Ashish Bharath
- Aryan Amit Barsainyan
- Amandeep Singh

### Members:
- Bharadwaja M Chittrapragada
- Karan Kumar Bhagat
- Mohammad Aadil Shabier
- Tejashree Chaudhari

## Aim
To deploy a model based on the F-Gen framework to generate an FAQ from multiple similar
sounding email queries.

## Introduction
With overwhelming volumes of official emails being exchanged in enterprises every day, emails
have become vital information storehouses. Automatic generation of FAQs from email systems
helps in identifying important information. Our project solves this problem using an F-Gen
framework. An F-Gen framework is a tool designed to convert emails into Frequently Asked
Questions (FAQs) by utilizing natural language processing techniques. The goal of the
framework is to reduce the time and effort required to manage and respond to customer emails,
while also improving the overall customer experience. The F-Gen framework includes three
subsystems connected end to end.

![image 1](\virtual-expo\assets\img\compsoc\Image1_Email-2-FAQ.jpeg)

## QC - Query Classifier Subsystem
Email sentences are taken as input and the data is cleaned and sentences are tokenized.
Tokenization involves splitting paragraphs and sentences into smaller units that can be more
easily assigned meaning. This preprocessed data is then fed into Bidirectional Encoder
Representations from Transformers (BERT) model which uses the Transformer encoder
architecture to process each token of input text in the full context of all tokens before and after it
in the sentence to classify queries.

## FGG - FAQ Group Generator Subsystem
Classified queries are fed to a Word2Vec pre-trained model . Word2vec is a two-layer neural
network used to generate distributed representations of words called word embeddings. Word
embeddings are a collection of numerical vectors (embeddings) that represent words such that
it captures their semantic meaning and context.

### Siamese LSTM
Siamese LSTM (Long Short Term Memory) network used to compare semantic similarity
between sentences(word embeddings).. It consists of two identical sub-networks that share the
same weights and architecture wherein each sub-network is an LSTM network.To train the
network, pairs of input sequences are fed into the two LSTMs, and their outputs are compared
using a distance metric. The network is trained to minimize the distance between similar
sequences and maximize the distance between dissimilar sequences. Semantic similarity is
represented on a numerical scale based on which similar sentences are grouped together.
A Siamese LSTM model was chosen to compare semantic similarity after accuracy scores of
other models like Cosine similarity, Jaccard, Levenshtein Distance and Transformer based
techniques were compared.

![image](\virtual-expo\assets\img\compsoc\Image2_Email-2-FAQ.png)

### Dataset used:
The Siamese LSTM model was trained on 10,000 questions .These questions were generated
using a Chat-GPT based API script. The dataset contained queries pertaining to various
industries like Laptop servicing, Tours and Travels, food delivery business, real estate builders

## FG - FAQ Generator Subsystem
A single FAQ is generated from multiple queries that had been grouped together by second
subsystem. This is carried out using RoBERTa(Robustly Optimized BERT Pre-training
Approach) Summarizer, Sumy and Gensim Summarizer.RoBERTa summarizer is a pre-trained
transformer based model while Sumy and Gensim are python libraries that include algorithms
for automatic text summarization. These algorithms analyze the contents of text and identify the
most important sentences which are used to generate a summary of the text. A single FAQ is
then generated using the summarized text.
![image](\virtual-expo\assets\img\compsoc\Image3_Email-2-FAQ.png)

## Python Libraries and Modules used
Numpy, pandas, matplotlib, keras, nltk, gensim, intertool, tensorflow, datasets, transformers,
sumy, typing, dataclasses , re, email, torch, seaborn, warnings, os, model, config, train,
data_loader, tqdm, logging, datetime, time

## Conclusion
- Successfully implemented an F-Gen Framework model which converts email queries to a
single FAQ.
- Accuracy of the Siamese LSTM model can be improved after training on larger datasets and
feeding a Thesaurus-like vocabulary to replace synonyms.

## References

1. [A deep learning based end-to-end system for automated email FAQ generation-ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0957417421012525?via%3Dihub)
2. [A Framework for Automatic Generation of FAQs from Email Repositories IEEE Conference Publication IEEE Xplore](https://ieeexplore.ieee.org/document/8614894)
3. [Siamese Recurrent Architectures for Learning Sentence Similarity Proceedings of the AAAI Conference on Artificial Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/10350)
4. [Domain Adapted word embeddings for improved sentiment classification](https://arxiv.org/abs/1805.04576)
5. [Graph embedding techniques, applications, and performance: A survey - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0950705118301540)